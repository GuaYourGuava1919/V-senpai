import os
import re
import uuid
import json
import fitz  # PyMuPDF
import cohere
from pinecone import Pinecone, ServerlessSpec
from dotenv import load_dotenv

# åŠ è¼‰ç’°å¢ƒè®Šæ•¸
load_dotenv()

# åˆå§‹åŒ– Cohere å®¢æˆ¶ç«¯
co = cohere.Client("nyJzgBpibkpcykwBIfvUij8bbyITEVcBJVqc2zvN")

# åˆå§‹åŒ– Pinecone å¯¦ä¾‹
pc = Pinecone(api_key="pcsk_574hwv_HimevaC5VKUfXogLB2e9g1Ufe8fEyxyWjxoiMfxvvGEux6u1A1ubrrssJkuz5mo")

# æª¢æŸ¥ index æ˜¯å¦å­˜åœ¨
if "myindex" not in pc.list_indexes().names():
    pc.create_index(
        name="myindex",
        dimension=1024,
        metric="cosine",
        spec=ServerlessSpec(cloud="aws", region="us-east-1")
    )

index = pc.Index("myindex")

# å®šç¾© PDF æ–‡å­—æå–å‡½æ•¸
def extract_text_from_pdf(pdf_path):
    doc = fitz.open(pdf_path)
    text = ""
    for page in doc:
        text += page.get_text()
    return text

# å®šç¾©éå•é¡Œå‰ç¶´
NON_QUESTION_PREFIXES = ["å—è¨ªè€…", "ç­ç´š", "å§“å", "å­¸è™Ÿ", "å­¸é•·å§", "è¨ªè«‡è€…", "ä¸»é¡Œ", "none", "n/a"]

# å®šç¾©å•é¡Œé©—è­‰å‡½æ•¸
def is_valid_question(q):
    q = q.strip().lower()
    if not q or len(q) < 3:
        return False
    if any(prefix in q for prefix in NON_QUESTION_PREFIXES):
        return False
    if re.match(r"^[a-zA-Z0-9\s\.:ï¼šã€ï¼Œã€‚()ï¼ˆï¼‰]+$", q) and len(q) < 10:
        return False
    return True

# å®šç¾©åˆ†å±¤æ“·å–å‡½æ•¸
def chunk_interview_text(text, source_file="unknown.pdf"):
    interviewee_match = re.search(r"(?:å—è¨ªè€…å§“å|è¨ªè«‡å­¸é•·å§“å|å—è¨ªè€…)[^\nï¼š:]*[:ï¼š]\s*([\u4e00-\u9fa5ã€A-Za-z\s]+)", text)
    current_interviewee = interviewee_match.group(1).strip() if interviewee_match else "æœªçŸ¥"

    # æ“·å–ä¸»é¡Œå¤§é¡Œï¼šä¾‹å¦‚ 1. å•é¡Œï¼Ÿ
    main_pattern = r"(\d+\.\s*.+?)(?=\n\d+\.\s|\Z)"
    main_matches = re.findall(main_pattern, text, re.DOTALL)

    chunks = []

    for main in main_matches:
        lines = main.strip().splitlines()
        main_question = lines[0].strip()
        main_answer = "\n".join(lines[1:]).strip()

        # æ”¹å–„å­é¡Œåˆ‡åˆ†ï¼šæ”¯æ´ (1) 1. 1ï¼‰ç­‰æ ¼å¼
        sub_pattern = r"(?:\n)?[ï¼ˆ(]?\s*(\d+)[.)ï¼‰]\s*"
        sub_matches = re.split(sub_pattern, main_answer)

        if len(sub_matches) > 2:
            for i in range(1, len(sub_matches)-1, 2):
                sub_num = sub_matches[i]
                sub_answer = sub_matches[i+1].strip()

                # ç‰¹åˆ¥è™•ç†å»¶ä¼¸å¿ƒå¾—åˆ‡åˆ†
                sub_chunks = re.split(r"\n(?=(?:å¿ƒå¾—|å»ºè­°|æ•´ç†|è£œå……|Q&A|æ³¨æ„äº‹é …)[ï¼š:])", sub_answer)
                for j, section in enumerate(sub_chunks):
                    q_label = f"{main_question} - å­é¡Œ {sub_num}"
                    if len(sub_chunks) > 1:
                        q_label += f"ï¼ˆæ®µè½ {j+1}ï¼‰"
                    if is_valid_question(q_label) and section:
                        chunks.append({
                            "chunk_id": str(uuid.uuid4()),
                            "interviewee": current_interviewee,
                            "question": q_label,
                            "answer": section.strip(),
                            "source_file": source_file
                        })
        else:
            # ç„¡å­é¡Œæ™‚ä¹Ÿè™•ç†å¿ƒå¾—å»¶ä¼¸æ®µè½
            sections = re.split(r"\n(?=(?:å¿ƒå¾—|å»ºè­°|æ•´ç†|è£œå……|Q&A|æ³¨æ„äº‹é …)[ï¼š:])", main_answer)
            for j, section in enumerate(sections):
                label = main_question
                if len(sections) > 1:
                    label += f"ï¼ˆæ®µè½ {j+1}ï¼‰"
                if is_valid_question(label) and section:
                    chunks.append({
                        "chunk_id": str(uuid.uuid4()),
                        "interviewee": current_interviewee,
                        "question": label,
                        "answer": section.strip(),
                        "source_file": source_file
                    })

    print(f"âœ… åˆ†å±¤æ“·å– {len(chunks)} ç­† chunkï¼ˆå—è¨ªè€…ï¼š{current_interviewee}ï¼Œæª”æ¡ˆï¼š{source_file}ï¼‰")
    if chunks:
        print(f"ç¯„ä¾‹å•é¡Œï¼š{chunks[0]['question']}")
        print(f"ç¯„ä¾‹å›ç­”ï¼š{chunks[0]['answer'][:60]}...")
    print("========================================")
    return chunks


# å®šç¾© LLM æ“·å–å‡½æ•¸
def llm_chunk(text, source_file="unknown.pdf"):
    prompt = f"""ä»¥ä¸‹æ˜¯ä¸€æ®µå—è¨ªç´€éŒ„ï¼Œè«‹ä½ å¾ä¸­æ‰¾å‡ºæ‰€æœ‰è¨ªè«‡å•é¡Œèˆ‡å°æ‡‰çš„å›ç­”ï¼Œè¼¸å‡ºæ ¼å¼ç‚º JSON é™£åˆ—ï¼Œæ¯ç­†åŒ…å«ï¼š
- questionï¼ˆå•é¡Œï¼‰
- answerï¼ˆå›ç­”ï¼‰

è«‹æ’é™¤ã€Œå—è¨ªè€…å§“åã€ã€ã€Œç­ç´šã€ã€ã€Œå­¸è™Ÿã€ç­‰ä¸æ˜¯å•é¡Œçš„æ¬„ä½ã€‚

æ–‡å­—å¦‚ä¸‹ï¼š
{text}

è«‹å›å‚³ JSON é™£åˆ—ï¼š
[
  {{
    "question": "...",
    "answer": "..."
  }},
  ...
]"""

    response = co.chat(
        model="command-r-plus",
        message=prompt,
        temperature=0.3,
        max_tokens=1200
    )

    try:
        parsed = json.loads(response.text)
        chunks = []
        for qa in parsed:
            if qa.get("question") and qa.get("answer") and is_valid_question(qa["question"]):
                chunks.append({
                    "chunk_id": str(uuid.uuid4()),
                    "interviewee": "æœªçŸ¥",
                    "question": qa["question"].strip(),
                    "answer": qa["answer"].strip(),
                    "source_file": source_file
                })
        return chunks
    except Exception as e:
        print("âŒ LLM å›å‚³æ ¼å¼éŒ¯èª¤ï¼š", str(e))
        return []

# å®šç¾©è‡ªå‹•é¸æ“‡æ“·å–æ–¹æ³•
def auto_chunk(text, source_file="unknown.pdf"):
    chunks = chunk_interview_text(text, source_file)
    if chunks:
        return chunks
    else:
        print(f"âš ï¸ Regex å¤±æ•—ï¼Œæ”¹ç”¨ Cohere LLM è¬ç”¨æŠ½å–ä¸­...")
        return llm_chunk(text, source_file)

# å®šç¾©è™•ç†è³‡æ–™å¤¾çš„å‡½æ•¸
def process_folder(folder_path, output_path="all_chunks.json"):
    all_chunks = []
    for filename in os.listdir(folder_path):
        if filename.lower().endswith(".pdf"):
            full_path = os.path.join(folder_path, filename)
            print(f"\nğŸ“„ æ­£åœ¨è™•ç†ï¼š{filename}")
            try:
                text = extract_text_from_pdf(full_path)
                chunks = auto_chunk(text, source_file=filename)
                if chunks:
                    all_chunks.extend(chunks)
                    print(f"âœ… å®Œæˆï¼š{filename}ï¼ˆ{len(chunks)} ç­† chunkï¼‰")
                else:
                    print(f"âš ï¸ ç„¡ chunkï¼š{filename}")
            except Exception as e:
                print(f"âŒ éŒ¯èª¤è™•ç† {filename}ï¼š{str(e)}")

    with open(output_path, "w", encoding="utf-8") as f:
        json.dump(all_chunks, f, ensure_ascii=False, indent=2)
    print(f"\nğŸ‰ å…± {len(all_chunks)} ç­†è³‡æ–™ï¼Œå·²å„²å­˜è‡³ {output_path}")

# å®šç¾©åµŒå…¥èˆ‡ä¸Šå‚³å‡½æ•¸
def get_embeddings(texts):
    response = co.embed(
        texts=texts,
        model="embed-multilingual-v3.0",
        input_type="search_document",
        embedding_types=["float"]
    )
    return response.embeddings.float

# å®šç¾©ä¸Šå‚³ chunks å‡½æ•¸
def upsert_chunks(chunks, namespace="interview"):
    from tqdm import tqdm

    existing_ids = set()
    ids_to_check = [c["chunk_id"] for c in chunks]

    # æ‰¹æ¬¡æŸ¥è©¢å·²å­˜åœ¨å‘é‡
    BATCH = 100
    for i in range(0, len(ids_to_check), BATCH):
        batch_ids = ids_to_check[i:i+BATCH]
        existing = index.fetch(ids=batch_ids, namespace=namespace)
        existing_ids.update(existing.vectors.keys())

    # éæ¿¾å·²å­˜åœ¨çš„ chunks
    new_chunks = [c for c in chunks if c["chunk_id"] not in existing_ids]
    if not new_chunks:
        print("â© æ‰€æœ‰ chunks éƒ½å·²å­˜åœ¨ï¼Œç„¡éœ€ä¸Šå‚³")
        return

    # é€²è¡ŒåµŒå…¥èˆ‡ä¸Šå‚³
    texts = [f"å•é¡Œï¼š{c['question']}\nå›ç­”ï¼š{c['answer']}" for c in new_chunks]
    embeddings = get_embeddings(texts)

    vectors = []
    for chunk, emb in zip(new_chunks, embeddings):
        vectors.append({
            "id": chunk["chunk_id"],
            "values": emb,
            "metadata": {
                "interviewee": chunk["interviewee"],
                "question": chunk["question"],
                "answer": chunk["answer"],
                "source_file": chunk["source_file"],
                "page_number": chunk.get("page_number", -1)
            }
        })

    index.upsert(vectors=vectors, namespace=namespace)
    print(f"âœ… ä¸Šå‚³ {len(vectors)} ç­†æ–°å‘é‡è‡³ Pineconeï¼ˆnamespace: {namespace}ï¼‰")


if __name__ == "__main__":
    folder_path = "C:\\Users\\Nicole\\Desktop\\vue-project\\files_backup"
    process_folder(folder_path)

    with open("all_chunks.json", "r", encoding="utf-8") as f:
        chunks = json.load(f)
        print(chunks)

    # upsert_chunks(chunks, namespace="interview")

