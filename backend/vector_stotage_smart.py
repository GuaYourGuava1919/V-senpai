import os
import re
import uuid
import json
import fitz  # PyMuPDF
import cohere
from pinecone import Pinecone, ServerlessSpec
from dotenv import load_dotenv

# åŠ è¼‰ç’°å¢ƒè®Šæ•¸
load_dotenv()

# åˆå§‹åŒ– Cohere å®¢æˆ¶ç«¯
co = cohere.Client("nyJzgBpibkpcykwBIfvUij8bbyITEVcBJVqc2zvN")

# åˆå§‹åŒ– Pinecone å¯¦ä¾‹
pc = Pinecone(api_key="pcsk_574hwv_HimevaC5VKUfXogLB2e9g1Ufe8fEyxyWjxoiMfxvvGEux6u1A1ubrrssJkuz5mo")

# æª¢æŸ¥ index æ˜¯å¦å­˜åœ¨
if "myindex" not in pc.list_indexes().names():
    pc.create_index(
        name="myindex",
        dimension=1024,
        metric="cosine",
        spec=ServerlessSpec(cloud="aws", region="us-east-1")
    )

index = pc.Index("myindex")

# å®šç¾© PDF æ–‡å­—æå–å‡½æ•¸
def extract_text_from_pdf(pdf_path):
    doc = fitz.open(pdf_path)
    text = ""
    for page in doc:
        text += page.get_text()
    return text

# å®šç¾©éå•é¡Œå‰ç¶´
NON_QUESTION_PREFIXES = ["å—è¨ªè€…", "ç­ç´š", "å§“å", "å­¸è™Ÿ", "å­¸é•·å§", "è¨ªè«‡è€…", "ä¸»é¡Œ", "none", "n/a"]

# å®šç¾©å•é¡Œé©—è­‰å‡½æ•¸
def is_valid_question(q):
    q = q.strip().lower()
    if not q or len(q) < 3:
        return False
    if any(prefix in q for prefix in NON_QUESTION_PREFIXES):
        return False
    if re.match(r"^[a-zA-Z0-9\s\.:ï¼šã€ï¼Œã€‚()ï¼ˆï¼‰]+$", q) and len(q) < 10:
        return False
    return True

# å¼·åŒ–ï¼šéæ¿¾ç•°å¸¸å­é¡Œè™Ÿã€åŠ å…¥ page_number èˆ‡å›ç­”é•·åº¦ã€è£œå…¨ LLM å•é¡Œæ¨™é¡Œ

def chunk_interview_text(text, source_file="unknown.pdf"):
    doc = fitz.open(source_file) if os.path.exists(source_file) else None
    page_map = {}
    if doc:
        for i, page in enumerate(doc):
            page_text = page.get_text()
            for line in page_text.splitlines():
                page_map[line.strip()] = i + 1

    interviewee_match = re.search(r"(?:å—è¨ªè€…å§“å|è¨ªè«‡å­¸é•·å§“å|å—è¨ªè€…)[^\nï¼š:]*[:ï¼š]\s*([\u4e00-\u9fa5ã€A-Za-z\s]+)", text)
    current_interviewee = interviewee_match.group(1).strip() if interviewee_match else "æœªçŸ¥"

    main_pattern = r"(\d+\.\s*.+?)(?=\n\d+\.\s|\Z)"
    main_matches = re.findall(main_pattern, text, re.DOTALL)
    chunks = []

    for main in main_matches:
        lines = main.strip().splitlines()
        main_question = lines[0].strip()
        main_answer = "\n".join(lines[1:]).strip()

        sub_pattern = r"(?:\n)?[ï¼ˆ(]?\s*(\d+)[.)ï¼‰]\s*"
        sub_matches = re.split(sub_pattern, main_answer)

        if len(sub_matches) > 2:
            for i in range(1, len(sub_matches)-1, 2):
                sub_num = sub_matches[i]
                if not sub_num.isdigit() or int(sub_num) > 50:
                    continue
                sub_answer = sub_matches[i+1].strip()
                sub_chunks = re.split(r"\n(?=(?:å¿ƒå¾—|å»ºè­°|æ•´ç†|è£œå……|Q&A|æ³¨æ„äº‹é …)[:ï¼š])", sub_answer)
                for j, section in enumerate(sub_chunks):
                    q_label = f"{main_question} - å­é¡Œ {sub_num}"
                    if len(sub_chunks) > 1:
                        q_label += f"ï¼ˆæ®µè½ {j+1}ï¼‰"
                    if is_valid_question(q_label) and section:
                        chunks.append({
                            "chunk_id": str(uuid.uuid4()),
                            "interviewee": current_interviewee,
                            "question": q_label,
                            "answer": section.strip(),
                            "source_file": source_file,
                            "page_number": page_map.get(section.strip().splitlines()[0], -1),
                            "answer_length": len(section.strip())
                        })
        else:
            sections = re.split(r"\n(?=(?:å¿ƒå¾—|å»ºè­°|æ•´ç†|è£œå……|Q&A|æ³¨æ„äº‹é …)[:ï¼š])", main_answer)
            for j, section in enumerate(sections):
                label = main_question
                if len(sections) > 1:
                    label += f"ï¼ˆæ®µè½ {j+1}ï¼‰"
                if is_valid_question(label) and section:
                    chunks.append({
                        "chunk_id": str(uuid.uuid4()),
                        "interviewee": current_interviewee,
                        "question": label,
                        "answer": section.strip(),
                        "source_file": source_file,
                        "page_number": page_map.get(section.strip().splitlines()[0], -1),
                        "answer_length": len(section.strip())
                    })
    return chunks


def llm_chunk(text, source_file="unknown.pdf"):
    prompt = f"""ä»¥ä¸‹æ˜¯ä¸€æ®µå—è¨ªç´€éŒ„ï¼Œè«‹ä½ å¾ä¸­æ‰¾å‡ºæ‰€æœ‰è¨ªè«‡å•é¡Œèˆ‡å°æ‡‰çš„å›ç­”ï¼Œè¼¸å‡ºæ ¼å¼ç‚º JSON é™£åˆ—ï¼Œæ¯ç­†åŒ…å«ï¼š
- questionï¼ˆå•é¡Œï¼‰
- answerï¼ˆå›ç­”ï¼‰

è‹¥ç™¼ç¾æ®µè½ä¸­åªæœ‰å›ç­”å…§å®¹ï¼Œè«‹æ ¹æ“šä¸Šä¸‹æ–‡åˆç†æ¨æ¸¬å°æ‡‰çš„å•é¡Œæ¨™é¡Œä¸¦è£œä¸Šã€‚è«‹æ’é™¤ã€Œå—è¨ªè€…å§“åã€ã€ã€Œç­ç´šã€ã€ã€Œå­¸è™Ÿã€ç­‰ä¸æ˜¯å•é¡Œçš„æ¬„ä½ã€‚

æ–‡å­—å¦‚ä¸‹ï¼š
{text}

è«‹å›å‚³ JSON é™£åˆ—ï¼š
[
  {{
    "question": "...",
    "answer": "..."
  }},
  ...
]"""
    response = co.chat(model="command-r-plus", message=prompt, temperature=0.3, max_tokens=1200)
    try:
        parsed = json.loads(response.text)
        return [
            {
                "chunk_id": str(uuid.uuid4()),
                "interviewee": "æœªçŸ¥",
                "question": qa["question"].strip(),
                "answer": qa["answer"].strip(),
                "source_file": source_file,
                "page_number": -1,
                "answer_length": len(qa["answer"].strip())
            }
            for qa in parsed
            if qa.get("question") and qa.get("answer") and is_valid_question(qa["question"])
        ]
    except Exception as e:
        print("âŒ LLM å›å‚³æ ¼å¼éŒ¯èª¤ï¼š", str(e))
        return []


# è‡ªå‹•é¸æ“‡æ“·å–æ–¹å¼ä¸¦æ¨™è¨˜æ–¹æ³•
def auto_chunk(text, source_file="unknown.pdf"):
    chunks = chunk_interview_text(text, source_file)
    method = "regex"
    if not chunks:
        print(f"âš ï¸ Regex æ“·å–å¤±æ•—ï¼Œæ”¹ç”¨ Cohere LLM è¬ç”¨æŠ½å–ï¼š{source_file}")
        chunks = llm_chunk(text, source_file)
        method = "llm" if chunks else "fail"
    return chunks, method

# è™•ç†æ•´å€‹è³‡æ–™å¤¾ã€ç”¢å‡ºå ±å‘Š
def process_folder_with_report(folder_path, output_path="all_chunks.json"):
    all_chunks = []
    report = []

    for filename in os.listdir(folder_path):
        if filename.lower().endswith(".pdf"):
            full_path = os.path.join(folder_path, filename)
            print(f"\nğŸ“„ æ­£åœ¨è™•ç†ï¼š{filename}")
            try:
                text = extract_text_from_pdf(full_path)
                chunks, method = auto_chunk(text, source_file=filename)
                all_chunks.extend(chunks)
                report.append({
                    "filename": filename,
                    "text_length": len(text),
                    "chunk_count": len(chunks),
                    "used_method": method,
                    "sample_question": chunks[0]["question"] if chunks else None
                })
            except Exception as e:
                report.append({
                    "filename": filename,
                    "text_length": 0,
                    "chunk_count": 0,
                    "used_method": f"error: {str(e)}",
                    "sample_question": None
                })

    with open(output_path, "w", encoding="utf-8") as f:
        json.dump(all_chunks, f, ensure_ascii=False, indent=2)
    print(f"\nğŸ‰ å…± {len(all_chunks)} ç­†è³‡æ–™ï¼Œå·²å„²å­˜è‡³ {output_path}")

    with open("chunk_report.json", "w", encoding="utf-8") as f:
        json.dump(report, f, ensure_ascii=False, indent=2)
    print("ğŸ“Š å ±å‘Šå·²å„²å­˜è‡³ chunk_report.json")

    return all_chunks, report

# åµŒå…¥èˆ‡ä¸Šå‚³å‡½æ•¸
def get_embeddings(texts):
    response = co.embed(
        texts=texts,
        model="embed-multilingual-v3.0",
        input_type="search_document",
        embedding_types=["float"]
    )
    return response.embeddings.float

def upsert_chunks(chunks, namespace="interview"):
    from tqdm import tqdm
    existing_ids = set()
    ids_to_check = [c["chunk_id"] for c in chunks]

    BATCH = 100
    for i in range(0, len(ids_to_check), BATCH):
        batch_ids = ids_to_check[i:i+BATCH]
        existing = index.fetch(ids=batch_ids, namespace=namespace)
        existing_ids.update(existing.vectors.keys())

    new_chunks = [c for c in chunks if c["chunk_id"] not in existing_ids]
    if not new_chunks:
        print("â© æ‰€æœ‰ chunks éƒ½å·²å­˜åœ¨ï¼Œç„¡éœ€ä¸Šå‚³")
        return

    texts = [f"å•é¡Œï¼š{c['question']}\nå›ç­”ï¼š{c['answer']}" for c in new_chunks]
    embeddings = get_embeddings(texts)

    vectors = [
        {
            "id": chunk["chunk_id"],
            "values": emb,
            "metadata": {
                "interviewee": chunk["interviewee"],
                "question": chunk["question"],
                "answer": chunk["answer"],
                "source_file": chunk["source_file"],
                "page_number": chunk.get("page_number", -1)
            }
        }
        for chunk, emb in zip(new_chunks, embeddings)
    ]

    index.upsert(vectors=vectors, namespace=namespace)
    print(f"âœ… ä¸Šå‚³ {len(vectors)} ç­†æ–°å‘é‡è‡³ Pineconeï¼ˆnamespace: {namespace}ï¼‰")

# ä¸»ç¨‹å¼å…¥å£
if __name__ == "__main__":
    folder_path = "C:\\Users\\Nicole\\Desktop\\vue-project\\files_backup"
    chunks, report = process_folder_with_report(folder_path)

    with open("all_chunks.json", "r", encoding="utf-8") as f:
        chunks = json.load(f)
    print(chunks)

    # å¦‚éœ€ä¸Šå‚³å‘é‡å¯å–æ¶ˆè¨»è§£
    # upsert_chunks(chunks, namespace="interview")
